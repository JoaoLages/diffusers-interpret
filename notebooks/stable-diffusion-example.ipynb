{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d21501a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a href=\"https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JoaoLages/diffusers-interpret/blob/main/notebooks/stable_diffusion_example_colab.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "# Stable Diffusion ðŸŽ¨ \n",
    "\n",
    "This notebook shows an example of how to run `diffusers_interpret.StableDiffusionPipelineExplainer` to explain `diffusers.StableDiffusionPipeline`.\n",
    "\n",
    "Before going through it, it is recommended to have a look at [ðŸ¤— HuggingFace's notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_diffusion.ipynb#scrollTo=-xMJ6LaET6dT)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6105914d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 0 - Login in HuggingFace's Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91153596",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "973ba5ac5987470798491c9ce9b5c2d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f3b7e3",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1 - Initialize StableDiffusionPipeline normally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# make sure you're logged in by running the previous cell or `huggingface-cli login`\n",
    "import torch\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from contextlib import nullcontext\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    \"CompVis/stable-diffusion-v1-4\", \n",
    "    use_auth_token=True,\n",
    "    \n",
    "    # FP16 is not working for 'cpu'\n",
    "    revision='fp16' if device != 'cpu' else None,\n",
    "    torch_dtype=torch.float16 if device != 'cpu' else None\n",
    ").to(device)\n",
    "pipe.enable_attention_slicing() # comment this line if you wish to deactivate this option"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2 - Pass StableDiffusionPipeline to StableDiffusionPipelineExplainer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from diffusers_interpret import StableDiffusionPipelineExplainer\n",
    "\n",
    "explainer = StableDiffusionPipelineExplainer(\n",
    "    pipe,\n",
    "    \n",
    "    # We pass `True` in here to be able to have a higher `n_last_diffusion_steps_to_consider_for_attributions` in the cell below\n",
    "    gradient_checkpointing=True \n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3 - Generate an image with the StableDiffusionPipelineExplainer object\n",
    "\n",
    "Note that the `explainer()` method accepts all the arguments that `pipe()` accepts. \n",
    "\n",
    "We also pass a `generator` argument so that we get a deterministic output."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "prompt = \"A cute corgi with the Eiffel Tower in the background\"\n",
    "\n",
    "generator = torch.Generator(device).manual_seed(2023)\n",
    "with torch.autocast('cuda') if device == 'cuda' else nullcontext():\n",
    "    output = explainer(\n",
    "        prompt, \n",
    "        num_inference_steps=50, \n",
    "        generator=generator,\n",
    "        height=448,\n",
    "        width=448,\n",
    "        \n",
    "        # for this model, the GPU VRAM usage will raise drastically if we increase this argument. feel free to experiment with it\n",
    "        # if you are not interested in checking the token attributions, you can pass 0 in here\n",
    "        n_last_diffusion_steps_to_consider_for_attributions=5\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3.1 - Check final generated image"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Final image\n",
    "output.image"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3.2 -  Check all the generated images during the diffusion process"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "output.all_images_during_generation.show(width=\"100%\", height=\"400px\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can also check the images individually:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Image at first generation \n",
    "output.all_images_during_generation[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Image at 33rd generation \n",
    "output.all_images_during_generation[33]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3.3 - Check the normalized and unnormalized token attributions \n",
    "\n",
    "We are now able to see what were the importances of each token in the input text to generate when generating the image.\n",
    "\n",
    "The token `corgi` was the most important feature according to our explainability method."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# (token, attribution)\n",
    "output.token_attributions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# (token, attribution_percentage)\n",
    "output.normalized_token_attributions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4 - Get explanations for a specific part of the image\n",
    "\n",
    "`diffusers-interpret` also computes the tokens importances for generating a particular part of the output image.\n",
    "\n",
    "In the current implementation, we only need to re-run the `explainer` and pass it the `explanation_2d_bounding_box` argument with the bounding box we are interested in seeing."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "prompt = \"A cute corgi with the Eiffel Tower in the background\"\n",
    "\n",
    "generator = torch.Generator(device).manual_seed(2023)\n",
    "with torch.autocast('cuda') if device == 'cuda' else nullcontext():\n",
    "    output = explainer(\n",
    "        prompt, \n",
    "        num_inference_steps=50, \n",
    "        generator=generator,\n",
    "        height=448,\n",
    "        width=448,\n",
    "        \n",
    "        # for this model, the GPU VRAM usage will raise drastically if we increase this argument. feel free to experiment with it\n",
    "        # if you are not interested in checking the token attributions, you can pass 0 in here\n",
    "        n_last_diffusion_steps_to_consider_for_attributions=5,\n",
    "        \n",
    "        explanation_2d_bounding_box=((305, 180), (448, 448)), # (upper left corner, bottom right corner)\n",
    "    )\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 4.1 - Check generated image\n",
    "\n",
    "A red bounding box is now visible in the picture, to indicate the area that `explainer` is looking at when calculating the token attributions."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "output.image"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 4.2 - Check token attributions for bounding box"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# (token, attribution_percentage)\n",
    "output.normalized_token_attributions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5 - Same generation, but with a different `explanation_2d_bounding_box`"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "prompt = \"A cute corgi with the Eiffel Tower in the background\"\n",
    "\n",
    "generator = torch.Generator(device).manual_seed(2023)\n",
    "with torch.autocast('cuda') if device == 'cuda' else nullcontext():\n",
    "    output = explainer(\n",
    "        prompt, \n",
    "        num_inference_steps=50, \n",
    "        generator=generator,\n",
    "        height=448,\n",
    "        width=448,\n",
    "        \n",
    "        # for this model, the GPU VRAM usage will raise drastically if we increase this argument. feel free to experiment with it\n",
    "        # if you are not interested in checking the token attributions, you can pass 0 in here\n",
    "        n_last_diffusion_steps_to_consider_for_attributions=5,\n",
    "        \n",
    "        explanation_2d_bounding_box=((140, 0), (270, 190)), # (upper left corner, bottom right corner)\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5 - Same generation, but with a different `explanation_2d_bounding_box`"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "prompt = \"A cute corgi with the Eiffel Tower in the background\"\n",
    "\n",
    "generator = torch.Generator(device).manual_seed(2023)\n",
    "with torch.autocast('cuda') if device == 'cuda' else nullcontext():\n",
    "    output = explainer(\n",
    "        prompt, \n",
    "        num_inference_steps=50, \n",
    "        generator=generator,\n",
    "        height=448,\n",
    "        width=448,\n",
    "        \n",
    "        # for this model, the GPU VRAM usage will raise drastically if we increase this argument. feel free to experiment with it\n",
    "        # if you are not interested in checking the token attributions, you can pass 0 in here\n",
    "        n_last_diffusion_steps_to_consider_for_attributions=5,\n",
    "        \n",
    "        explanation_2d_bounding_box=((140, 0), (270, 190)), # (upper left corner, bottom right corner)\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "output.sample"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# (token, attribution_percentage)\n",
    "output.normalized_token_attributions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8769758e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}