{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d21501a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a href=\"https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JoaoLages/diffusers-interpret/blob/main/notebooks/stable_diffusion_example_colab.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "# Stable Diffusion ðŸŽ¨ \n",
    "\n",
    "This notebook shows an example of how to run `diffusers_interpret.StableDiffusionPipelineExplainer` to explain `diffusers.StableDiffusionPipeline`.\n",
    "\n",
    "Before going through it, it is recommended to have a look at [ðŸ¤— HuggingFace's notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_diffusion.ipynb#scrollTo=-xMJ6LaET6dT)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6105914d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 0 - Login in HuggingFace's Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91153596",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a8089e0f0b0488a80e803f65522de69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f3b7e3",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1 - Initialize StableDiffusionPipeline normally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b34724b4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{'trained_betas'} was not found in config. Values will be initialized to default values.\n"
     ]
    }
   ],
   "source": [
    "# make sure you're logged in by running the previous cell or `huggingface-cli login`\n",
    "import torch\n",
    "from contextlib import nullcontext\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "from diffusers import StableDiffusionImg2ImgPipeline\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "pipe = StableDiffusionImg2ImgPipeline.from_pretrained(\n",
    "    \"CompVis/stable-diffusion-v1-4\", \n",
    "    use_auth_token=True,\n",
    "    \n",
    "    # FP16 is not working for 'cpu'\n",
    "    revision='fp16' if device != 'cpu' else None,\n",
    "    torch_dtype=torch.float16 if device != 'cpu' else None\n",
    ").to(device)\n",
    "pipe.enable_attention_slicing() # comment this line if you wish to deactivate this option"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8762d0",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2 - Pass StableDiffusionPipeline to StableDiffusionPipelineExplainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc068651",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from diffusers_interpret import StableDiffusionImg2ImgPipelineExplainer\n",
    "\n",
    "explainer = StableDiffusionImg2ImgPipelineExplainer(\n",
    "    pipe,\n",
    "    \n",
    "    # We pass `True` in here to be able to have a higher `n_last_diffusion_steps_to_consider_for_attributions` in the cell below\n",
    "    gradient_checkpointing=True \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa0157b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 3 - Generate an image with the StableDiffusionPipelineExplainer object\n",
    "\n",
    "Note that the `explainer()` method accepts all the arguments that `pipe()` accepts. \n",
    "\n",
    "We also pass a `generator` argument so that we get a deterministic output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed14c523",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.018483877182006836,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 45,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 38,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "097f61fbc1c24a8a98509c5d229ef96e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating token attributions... Done!\n",
      "Calculating image pixel attributions... > \u001b[0;32m/home/ubuntu/diffusers-interpret/src/diffusers_interpret/explainer.py\u001b[0m(380)\u001b[0;36m_get_attributions\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    379 \u001b[0;31m        \u001b[0;32mimport\u001b[0m \u001b[0mipdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mipdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 380 \u001b[0;31m        pixel_attributions = gradient_x_inputs_attribution(\n",
      "\u001b[0m\u001b[0;32m    381 \u001b[0;31m            \u001b[0mpred_logits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minit_image\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> c\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "RuntimeError: Attempt to retrieve a tensor saved by autograd multiple times without checkpoint recomputation being triggered in between, this is not currently supported. Please open an issue with details on your use case so that we can prioritize adding this.\n\nAt:\n  /home/ubuntu/diffusers-interpret/venv/lib/python3.10/site-packages/torch/utils/checkpoint.py(374): unpack\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m init_image \u001b[38;5;241m=\u001b[39m init_image\u001b[38;5;241m.\u001b[39mresize((\u001b[38;5;241m384\u001b[39m, \u001b[38;5;241m256\u001b[39m))\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautocast(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m nullcontext():\n\u001b[0;32m---> 11\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mexplainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_image\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrength\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.75\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mguidance_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m7.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#n_last_diffusion_steps_to_consider_for_attributions=5\u001b[39;49;00m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/diffusers-interpret/src/diffusers_interpret/explainer.py:147\u001b[0m, in \u001b[0;36mCorePipelineExplainer.__call__\u001b[0;34m(self, prompt, attribution_method, explanation_2d_bounding_box, consider_special_tokens, clean_token_prefixes_and_suffixes, run_safety_checker, n_last_diffusion_steps_to_consider_for_attributions, get_images_for_all_inference_steps, output_type, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;66;03m# Calculate primary attribution scores\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m calculate_attributions \u001b[38;5;129;01mand\u001b[39;00m attribution_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgrad_x_input\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 147\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_attributions\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexplanation_2d_bounding_box\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexplanation_2d_bounding_box\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconsider_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconsider_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclean_token_prefixes_and_suffixes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclean_token_prefixes_and_suffixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnly `attribution_method=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgrad_x_input\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m` is implemented for now\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/diffusers-interpret/src/diffusers_interpret/explainer.py:380\u001b[0m, in \u001b[0;36mBasePipelineImg2ImgExplainer._get_attributions\u001b[0;34m(self, output, tokens, text_embeddings, explanation_2d_bounding_box, consider_special_tokens, clean_token_prefixes_and_suffixes, retain_graph, **kwargs)\u001b[0m\n\u001b[1;32m    378\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalculating image pixel attributions... \u001b[39m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mipdb\u001b[39;00m; ipdb\u001b[38;5;241m.\u001b[39mset_trace()\n\u001b[0;32m--> 380\u001b[0m pixel_attributions \u001b[38;5;241m=\u001b[39m \u001b[43mgradient_x_inputs_attribution\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpred_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_image\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexplanation_2d_bounding_box\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexplanation_2d_bounding_box\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretain_graph\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mipdb\u001b[39;00m; ipdb\u001b[38;5;241m.\u001b[39mset_trace()\n",
      "File \u001b[0;32m~/diffusers-interpret/src/diffusers_interpret/attribution.py:30\u001b[0m, in \u001b[0;36mgradient_x_inputs_attribution\u001b[0;34m(pred_logits, input_embeds, explanation_2d_bounding_box, retain_graph)\u001b[0m\n\u001b[1;32m     27\u001b[0m tuple_of_pred_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(tuple_of_pred_logits)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# get the sum of back-prop gradients for all predictions with respect to the inputs\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m grad \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtuple_of_pred_logits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_embeds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretain_graph\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Grad X Input\u001b[39;00m\n\u001b[1;32m     33\u001b[0m grad_x_input \u001b[38;5;241m=\u001b[39m grad \u001b[38;5;241m*\u001b[39m input_embeds\n",
      "File \u001b[0;32m~/diffusers-interpret/venv/lib/python3.10/site-packages/torch/autograd/__init__.py:276\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _vmap_internals\u001b[38;5;241m.\u001b[39m_vmap(vjp, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, allow_none_pass_through\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(grad_outputs_)\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    277\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: RuntimeError: Attempt to retrieve a tensor saved by autograd multiple times without checkpoint recomputation being triggered in between, this is not currently supported. Please open an issue with details on your use case so that we can prioritize adding this.\n\nAt:\n  /home/ubuntu/diffusers-interpret/venv/lib/python3.10/site-packages/torch/utils/checkpoint.py(374): unpack\n"
     ]
    }
   ],
   "source": [
    "prompt = \"A fantasy landscape, trending on artstation\"\n",
    "\n",
    "# let's download an initial image\n",
    "url = \"https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/sketch-mountains-input.jpg\"\n",
    "\n",
    "response = requests.get(url)\n",
    "init_image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "init_image = init_image.resize((384, 256))\n",
    "\n",
    "with torch.autocast('cuda') if device == 'cuda' else nullcontext():\n",
    "    output = explainer(\n",
    "        prompt=prompt, init_image=init_image, strength=0.75, guidance_scale=7.5,\n",
    "        #n_last_diffusion_steps_to_consider_for_attributions=5\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66533c6a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 3.1 - Check final generated image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdec343",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Final image\n",
    "output.image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4275bd3c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 3.2 -  Check all the generated images during the diffusion process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d6761e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "output.all_images_during_generation.show(width=\"100%\", height=\"400px\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6389b971",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "You can also check the images individually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a466c77",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Image at first generation \n",
    "output.all_images_during_generation[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39843e1e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Image at 33rd generation \n",
    "output.all_images_during_generation[33]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d26146",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 3.3 - Check the normalized and unnormalized token attributions \n",
    "\n",
    "We are now able to see what were the importances of each token in the input text to generate when generating the image.\n",
    "\n",
    "The token `corgi` was the most important feature according to our explainability method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fe1db0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# (token, attribution)\n",
    "output.token_attributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8701bd4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# (token, attribution_percentage)\n",
    "output.normalized_token_attributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8625738",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 4 - Get explanations for a specific part of the image\n",
    "\n",
    "`diffusers-interpret` also computes the tokens importances for generating a particular part of the output image.\n",
    "\n",
    "In the current implementation, we only need to re-run the `explainer` and pass it the `explanation_2d_bounding_box` argument with the bounding box we are interested in seeing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58aa8d3b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"A cute corgi with the Eiffel Tower in the background\"\n",
    "\n",
    "generator = torch.Generator(device).manual_seed(2023)\n",
    "with torch.autocast('cuda') if device == 'cuda' else nullcontext():\n",
    "    output = explainer(\n",
    "        prompt, \n",
    "        num_inference_steps=50, \n",
    "        generator=generator,\n",
    "        height=448,\n",
    "        width=448,\n",
    "        \n",
    "        # for this model, the GPU VRAM usage will raise drastically if we increase this argument. feel free to experiment with it\n",
    "        # if you are not interested in checking the token attributions, you can pass 0 in here\n",
    "        n_last_diffusion_steps_to_consider_for_attributions=5,\n",
    "        \n",
    "        explanation_2d_bounding_box=((305, 180), (448, 448)), # (upper left corner, bottom right corner)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc600d82",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 4.1 - Check generated image\n",
    "\n",
    "A red bounding box is now visible in the picture, to indicate the area that `explainer` is looking at when calculating the token attributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9d0d3c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "output.image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b0f7fb",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 4.2 - Check token attributions for bounding box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de5f55d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# (token, attribution_percentage)\n",
    "output.normalized_token_attributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1026b0c3",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 5 - Same generation, but with a different `explanation_2d_bounding_box`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f371a01",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"A cute corgi with the Eiffel Tower in the background\"\n",
    "\n",
    "generator = torch.Generator(device).manual_seed(2023)\n",
    "with torch.autocast('cuda') if device == 'cuda' else nullcontext():\n",
    "    output = explainer(\n",
    "        prompt, \n",
    "        num_inference_steps=50, \n",
    "        generator=generator,\n",
    "        height=448,\n",
    "        width=448,\n",
    "        \n",
    "        # for this model, the GPU VRAM usage will raise drastically if we increase this argument. feel free to experiment with it\n",
    "        # if you are not interested in checking the token attributions, you can pass 0 in here\n",
    "        n_last_diffusion_steps_to_consider_for_attributions=5,\n",
    "        \n",
    "        explanation_2d_bounding_box=((140, 0), (270, 190)), # (upper left corner, bottom right corner)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa11d228",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 5 - Same generation, but with a different `explanation_2d_bounding_box`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e55c6c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"A cute corgi with the Eiffel Tower in the background\"\n",
    "\n",
    "generator = torch.Generator(device).manual_seed(2023)\n",
    "with torch.autocast('cuda') if device == 'cuda' else nullcontext():\n",
    "    output = explainer(\n",
    "        prompt, \n",
    "        num_inference_steps=50, \n",
    "        generator=generator,\n",
    "        height=448,\n",
    "        width=448,\n",
    "        \n",
    "        # for this model, the GPU VRAM usage will raise drastically if we increase this argument. feel free to experiment with it\n",
    "        # if you are not interested in checking the token attributions, you can pass 0 in here\n",
    "        n_last_diffusion_steps_to_consider_for_attributions=5,\n",
    "        \n",
    "        explanation_2d_bounding_box=((140, 0), (270, 190)), # (upper left corner, bottom right corner)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce463bab",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "output.image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04443d31",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# (token, attribution_percentage)\n",
    "output.normalized_token_attributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8769758e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
