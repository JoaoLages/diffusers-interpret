{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d21501a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a href=\"https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JoaoLages/diffusers-interpret/blob/main/notebooks/stable_diffusion_example_colab.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "# Stable Diffusion ðŸŽ¨ \n",
    "\n",
    "This notebook shows an example of how to run `diffusers_interpret.StableDiffusionPipelineExplainer` to explain `diffusers.StableDiffusionPipeline`.\n",
    "\n",
    "Before going through it, it is recommended to have a look at [ðŸ¤— HuggingFace's notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_diffusion.ipynb#scrollTo=-xMJ6LaET6dT)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6105914d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 0 - Login in HuggingFace's Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91153596",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec2cb871112e40e5b4e69ff1da0455e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f3b7e3",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1 - Initialize StableDiffusionPipeline normally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b34724b4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{'trained_betas'} was not found in config. Values will be initialized to default values.\n"
     ]
    }
   ],
   "source": [
    "# make sure you're logged in by running the previous cell or `huggingface-cli login`\n",
    "import torch\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from contextlib import nullcontext\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    \"CompVis/stable-diffusion-v1-4\", \n",
    "    use_auth_token=True,\n",
    "    \n",
    "    # FP16 is not working for 'cpu'\n",
    "    revision='fp16' if device != 'cpu' else None,\n",
    "    torch_dtype=torch.float16 if device != 'cpu' else None\n",
    ").to(device)\n",
    "pipe.enable_attention_slicing() # comment this line if you wish to deactivate this option"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8762d0",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2 - Pass StableDiffusionPipeline to StableDiffusionPipelineExplainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc068651",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from diffusers_interpret import StableDiffusionPipelineExplainer\n",
    "\n",
    "explainer = StableDiffusionPipelineExplainer(\n",
    "    pipe,\n",
    "    \n",
    "    # We pass `True` in here to be able to have a higher `n_last_diffusion_steps_to_consider_for_attributions` in the cell below\n",
    "    gradient_checkpointing=True \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa0157b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 3 - Generate an image with the StableDiffusionPipelineExplainer object\n",
    "\n",
    "Note that the `explainer()` method accepts all the arguments that `pipe()` accepts. \n",
    "\n",
    "We also pass a `generator` argument so that we get a deterministic output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed14c523",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.014973878860473633,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 45,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 51,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2a727d44ddf4b3eb2da1639aff170e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating token attributions... "
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "RuntimeError: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 0; 14.76 GiB total capacity; 3.47 GiB already allocated; 8.75 MiB free; 3.55 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n\nAt:\n  /home/ubuntu/diffusers-interpret/venv/lib/python3.10/site-packages/torch/nn/functional.py(2516): group_norm\n  /home/ubuntu/diffusers-interpret/venv/lib/python3.10/site-packages/torch/nn/modules/normalization.py(272): forward\n  /home/ubuntu/diffusers-interpret/venv/lib/python3.10/site-packages/torch/nn/modules/module.py(1130): _call_impl\n  /home/ubuntu/diffusers-interpret/venv/lib/python3.10/site-packages/diffusers/models/resnet.py(336): forward\n  /home/ubuntu/diffusers-interpret/venv/lib/python3.10/site-packages/torch/nn/modules/module.py(1130): _call_impl\n  /home/ubuntu/diffusers-interpret/venv/lib/python3.10/site-packages/diffusers/models/unet_blocks.py(1202): forward\n  /home/ubuntu/diffusers-interpret/venv/lib/python3.10/site-packages/torch/nn/modules/module.py(1130): _call_impl\n  /home/ubuntu/diffusers-interpret/venv/lib/python3.10/site-packages/diffusers/models/vae.py(202): forward\n  /home/ubuntu/diffusers-interpret/venv/lib/python3.10/site-packages/torch/nn/modules/module.py(1130): _call_impl\n  /home/ubuntu/diffusers-interpret/venv/lib/python3.10/site-packages/diffusers/models/vae.py(552): decode\n  /home/ubuntu/diffusers-interpret/venv/lib/python3.10/site-packages/torch/utils/checkpoint.py(371): unpack\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m generator \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mGenerator(device)\u001b[38;5;241m.\u001b[39mmanual_seed(\u001b[38;5;241m2023\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautocast(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m nullcontext():\n\u001b[0;32m----> 5\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mexplainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_inference_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m448\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwidth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m448\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# for this model, the GPU VRAM usage will raise drastically if we increase this argument. feel free to experiment with it\u001b[39;49;00m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# if you are not interested in checking the token attributions, you can pass 0 in here\u001b[39;49;00m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_last_diffusion_steps_to_consider_for_attributions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/diffusers-interpret/src/diffusers_interpret/explainer.py:120\u001b[0m, in \u001b[0;36mCorePipelineExplainer.__call__\u001b[0;34m(self, prompt, attribution_method, explanation_2d_bounding_box, consider_special_tokens, clean_token_prefixes_and_suffixes, run_safety_checker, n_last_diffusion_steps_to_consider_for_attributions, get_images_for_all_inference_steps, output_type, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m# Calculate primary attribution scores\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m calculate_attributions:\n\u001b[0;32m--> 120\u001b[0m     output: Union[PipelineExplainerOutput, PipelineImg2ImgExplainerOutput] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_attributions\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattribution_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattribution_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexplanation_2d_bounding_box\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexplanation_2d_bounding_box\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconsider_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconsider_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclean_token_prefixes_and_suffixes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclean_token_prefixes_and_suffixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_last_diffusion_steps_to_consider_for_attributions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_last_diffusion_steps_to_consider_for_attributions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_size \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;66;03m# squash batch dimension\u001b[39;00m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_attributions\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnormalized_token_attributions\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpixel_attributions\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    135\u001b[0m               \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnormalized_pixel_attributions\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n",
      "File \u001b[0;32m~/diffusers-interpret/src/diffusers_interpret/explainer.py:220\u001b[0m, in \u001b[0;36mCorePipelineExplainer._get_attributions\u001b[0;34m(self, output, attribution_method, tokens, text_embeddings, explanation_2d_bounding_box, consider_special_tokens, clean_token_prefixes_and_suffixes, n_last_diffusion_steps_to_consider_for_attributions, **kwargs)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose:\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalculating token attributions... \u001b[39m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 220\u001b[0m token_attributions \u001b[38;5;241m=\u001b[39m \u001b[43mgradients_attribution\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpred_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattribution_algorithms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mattribution_method\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokens_attribution_method\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexplanation_2d_bounding_box\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexplanation_2d_bounding_box\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    227\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post_process_token_attributions(\n\u001b[1;32m    228\u001b[0m     output\u001b[38;5;241m=\u001b[39moutput,\n\u001b[1;32m    229\u001b[0m     tokens\u001b[38;5;241m=\u001b[39mtokens,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    232\u001b[0m     clean_token_prefixes_and_suffixes\u001b[38;5;241m=\u001b[39mclean_token_prefixes_and_suffixes\n\u001b[1;32m    233\u001b[0m )\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose:\n",
      "File \u001b[0;32m~/diffusers-interpret/src/diffusers_interpret/attribution.py:39\u001b[0m, in \u001b[0;36mgradients_attribution\u001b[0;34m(pred_logits, input_embeds, attribution_algorithms, explanation_2d_bounding_box, retain_graph)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_autocast_enabled():\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;66;03m# FP16 may cause NaN gradients https://github.com/pytorch/pytorch/issues/40497\u001b[39;00m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;66;03m# TODO: this is still an issue, the code below does not solve it\u001b[39;00m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautocast(input_embeds[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype, enabled\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m---> 39\u001b[0m         grads \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtuple_of_pred_logits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_embeds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretain_graph\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     41\u001b[0m     grads \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgrad(tuple_of_pred_logits, input_embeds, retain_graph\u001b[38;5;241m=\u001b[39mretain_graph)\n",
      "File \u001b[0;32m~/diffusers-interpret/venv/lib/python3.10/site-packages/torch/autograd/__init__.py:276\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _vmap_internals\u001b[38;5;241m.\u001b[39m_vmap(vjp, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, allow_none_pass_through\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(grad_outputs_)\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    277\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: RuntimeError: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 0; 14.76 GiB total capacity; 3.47 GiB already allocated; 8.75 MiB free; 3.55 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n\nAt:\n  /home/ubuntu/diffusers-interpret/venv/lib/python3.10/site-packages/torch/nn/functional.py(2516): group_norm\n  /home/ubuntu/diffusers-interpret/venv/lib/python3.10/site-packages/torch/nn/modules/normalization.py(272): forward\n  /home/ubuntu/diffusers-interpret/venv/lib/python3.10/site-packages/torch/nn/modules/module.py(1130): _call_impl\n  /home/ubuntu/diffusers-interpret/venv/lib/python3.10/site-packages/diffusers/models/resnet.py(336): forward\n  /home/ubuntu/diffusers-interpret/venv/lib/python3.10/site-packages/torch/nn/modules/module.py(1130): _call_impl\n  /home/ubuntu/diffusers-interpret/venv/lib/python3.10/site-packages/diffusers/models/unet_blocks.py(1202): forward\n  /home/ubuntu/diffusers-interpret/venv/lib/python3.10/site-packages/torch/nn/modules/module.py(1130): _call_impl\n  /home/ubuntu/diffusers-interpret/venv/lib/python3.10/site-packages/diffusers/models/vae.py(202): forward\n  /home/ubuntu/diffusers-interpret/venv/lib/python3.10/site-packages/torch/nn/modules/module.py(1130): _call_impl\n  /home/ubuntu/diffusers-interpret/venv/lib/python3.10/site-packages/diffusers/models/vae.py(552): decode\n  /home/ubuntu/diffusers-interpret/venv/lib/python3.10/site-packages/torch/utils/checkpoint.py(371): unpack\n"
     ]
    }
   ],
   "source": [
    "prompt = \"A cute corgi with the Eiffel Tower in the background\"\n",
    "\n",
    "generator = torch.Generator(device).manual_seed(2023)\n",
    "with torch.autocast('cuda') if device == 'cuda' else nullcontext():\n",
    "    output = explainer(\n",
    "        prompt, \n",
    "        num_inference_steps=50, \n",
    "        generator=generator,\n",
    "        height=448,\n",
    "        width=448,\n",
    "        \n",
    "        # for this model, the GPU VRAM usage will raise drastically if we increase this argument. feel free to experiment with it\n",
    "        # if you are not interested in checking the token attributions, you can pass 0 in here\n",
    "        n_last_diffusion_steps_to_consider_for_attributions=5\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66533c6a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 3.1 - Check final generated image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdec343",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Final image\n",
    "output.image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4275bd3c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 3.2 -  Check all the generated images during the diffusion process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d6761e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "output.all_images_during_generation.show(width=\"100%\", height=\"400px\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6389b971",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "You can also check the images individually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a466c77",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Image at first generation \n",
    "output.all_images_during_generation[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39843e1e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Image at 33rd generation \n",
    "output.all_images_during_generation[33]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d26146",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 3.3 - Check the normalized and unnormalized token attributions \n",
    "\n",
    "We are now able to see what were the importances of each token in the input text to generate when generating the image.\n",
    "\n",
    "The token `corgi` was the most important feature according to our explainability method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fe1db0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# (token, attribution)\n",
    "output.token_attributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8701bd4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# (token, attribution_percentage)\n",
    "output.normalized_token_attributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8625738",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 4 - Get explanations for a specific part of the image\n",
    "\n",
    "`diffusers-interpret` also computes the tokens importances for generating a particular part of the output image.\n",
    "\n",
    "In the current implementation, we only need to re-run the `explainer` and pass it the `explanation_2d_bounding_box` argument with the bounding box we are interested in seeing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58aa8d3b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"A cute corgi with the Eiffel Tower in the background\"\n",
    "\n",
    "generator = torch.Generator(device).manual_seed(2023)\n",
    "with torch.autocast('cuda') if device == 'cuda' else nullcontext():\n",
    "    output = explainer(\n",
    "        prompt, \n",
    "        num_inference_steps=50, \n",
    "        generator=generator,\n",
    "        height=448,\n",
    "        width=448,\n",
    "        \n",
    "        # for this model, the GPU VRAM usage will raise drastically if we increase this argument. feel free to experiment with it\n",
    "        # if you are not interested in checking the token attributions, you can pass 0 in here\n",
    "        n_last_diffusion_steps_to_consider_for_attributions=5,\n",
    "        \n",
    "        explanation_2d_bounding_box=((305, 180), (448, 448)), # (upper left corner, bottom right corner)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc600d82",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 4.1 - Check generated image\n",
    "\n",
    "A red bounding box is now visible in the picture, to indicate the area that `explainer` is looking at when calculating the token attributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9d0d3c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "output.image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b0f7fb",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 4.2 - Check token attributions for bounding box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de5f55d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# (token, attribution_percentage)\n",
    "output.normalized_token_attributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1026b0c3",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 5 - Same generation, but with a different `explanation_2d_bounding_box`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f371a01",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"A cute corgi with the Eiffel Tower in the background\"\n",
    "\n",
    "generator = torch.Generator(device).manual_seed(2023)\n",
    "with torch.autocast('cuda') if device == 'cuda' else nullcontext():\n",
    "    output = explainer(\n",
    "        prompt, \n",
    "        num_inference_steps=50, \n",
    "        generator=generator,\n",
    "        height=448,\n",
    "        width=448,\n",
    "        \n",
    "        # for this model, the GPU VRAM usage will raise drastically if we increase this argument. feel free to experiment with it\n",
    "        # if you are not interested in checking the token attributions, you can pass 0 in here\n",
    "        n_last_diffusion_steps_to_consider_for_attributions=5,\n",
    "        \n",
    "        explanation_2d_bounding_box=((140, 0), (270, 190)), # (upper left corner, bottom right corner)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa11d228",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 5 - Same generation, but with a different `explanation_2d_bounding_box`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e55c6c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"A cute corgi with the Eiffel Tower in the background\"\n",
    "\n",
    "generator = torch.Generator(device).manual_seed(2023)\n",
    "with torch.autocast('cuda') if device == 'cuda' else nullcontext():\n",
    "    output = explainer(\n",
    "        prompt, \n",
    "        num_inference_steps=50, \n",
    "        generator=generator,\n",
    "        height=448,\n",
    "        width=448,\n",
    "        \n",
    "        # for this model, the GPU VRAM usage will raise drastically if we increase this argument. feel free to experiment with it\n",
    "        # if you are not interested in checking the token attributions, you can pass 0 in here\n",
    "        n_last_diffusion_steps_to_consider_for_attributions=5,\n",
    "        \n",
    "        explanation_2d_bounding_box=((140, 0), (270, 190)), # (upper left corner, bottom right corner)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce463bab",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "output.image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04443d31",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# (token, attribution_percentage)\n",
    "output.normalized_token_attributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8769758e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
